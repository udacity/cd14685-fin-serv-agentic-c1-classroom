{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77ab9e49",
   "metadata": {},
   "source": [
    "# ğŸ“Š Data Exploration - SAR Processing System\n",
    "\n",
    "Welcome to Phase 1 of the Financial Services Agentic AI Project!\n",
    "\n",
    "In this notebook, you'll explore the financial data that forms the foundation of our SAR (Suspicious Activity Report) processing system. Understanding this data is crucial for designing effective Pydantic schemas and building your AI agents.\n",
    "\n",
    "## ğŸ¯ Learning Objectives\n",
    "- Understand the structure of customer, account, and transaction data\n",
    "- Identify relationships between different data entities\n",
    "- Recognize patterns that might indicate suspicious activity\n",
    "- Prepare for designing Pydantic schemas in `foundation_sar.py`\n",
    "\n",
    "## ğŸ“‹ Business Context\n",
    "Financial institutions are required by law to monitor customer transactions and report suspicious activities to authorities. This process involves:\n",
    "- **Customer Due Diligence**: Understanding who your customers are\n",
    "- **Transaction Monitoring**: Watching for unusual patterns\n",
    "- **Risk Assessment**: Evaluating the likelihood of financial crimes\n",
    "- **Regulatory Reporting**: Filing SARs when required"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1894634",
   "metadata": {},
   "source": [
    "## ğŸš€ Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4916bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 10)\n",
    "\n",
    "print(\"ğŸ“š Libraries loaded successfully!\")\n",
    "print(\"ğŸ¯ Ready to explore financial data for SAR processing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90555223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the three main data files\n",
    "customers_df = pd.read_csv('../data/customers.csv')\n",
    "accounts_df = pd.read_csv('../data/accounts.csv')\n",
    "transactions_df = pd.read_csv('../data/transactions.csv')\n",
    "\n",
    "print(\"ğŸ“Š Data loaded successfully!\")\n",
    "print(f\"ğŸ‘¥ Customers: {len(customers_df):,} records\")\n",
    "print(f\"ğŸ¦ Accounts: {len(accounts_df):,} records\")\n",
    "print(f\"ğŸ’³ Transactions: {len(transactions_df):,} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a81931",
   "metadata": {},
   "source": [
    "## ğŸ‘¥ Customer Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5570208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore customer data structure\n",
    "print(\"ğŸ“‹ Customer Data Overview\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Shape: {customers_df.shape}\")\n",
    "print(f\"Columns: {list(customers_df.columns)}\")\n",
    "print(\"\\nğŸ“Š First few records:\")\n",
    "customers_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3ebcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze customer data types and missing values\n",
    "print(\"ğŸ” Customer Data Quality Analysis\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nğŸ“Š Data Types:\")\n",
    "print(customers_df.dtypes)\n",
    "print(\"\\nâ“ Missing Values:\")\n",
    "print(customers_df.isnull().sum())\n",
    "print(\"\\nğŸ“ˆ Basic Statistics:\")\n",
    "customers_df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2474eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze risk ratings distribution\n",
    "print(\"âš ï¸ Customer Risk Rating Analysis\")\n",
    "print(\"=\" * 50)\n",
    "risk_counts = customers_df['risk_rating'].value_counts()\n",
    "print(risk_counts)\n",
    "\n",
    "# Visualize risk distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "risk_counts.plot(kind='bar', color=['green', 'orange', 'red'])\n",
    "plt.title('Customer Risk Rating Distribution')\n",
    "plt.ylabel('Number of Customers')\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.pie(risk_counts.values, labels=risk_counts.index, autopct='%1.1f%%', \n",
    "        colors=['green', 'orange', 'red'])\n",
    "plt.title('Risk Rating Percentage')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nğŸ’¡ Key Insights:\")\n",
    "print(f\"â€¢ {risk_counts['High']} customers ({risk_counts['High']/len(customers_df)*100:.1f}%) are High Risk\")\n",
    "print(f\"â€¢ These high-risk customers require enhanced monitoring\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52de5b18",
   "metadata": {},
   "source": [
    "### ğŸ“ TODO: Design CustomerData Schema\n",
    "\n",
    "Based on your exploration above, you'll need to create a Pydantic schema in `foundation_sar.py` that includes:\n",
    "\n",
    "```python\n",
    "class CustomerData(BaseModel):\n",
    "    customer_id: str = Field(..., description=\"Unique customer identifier\")\n",
    "    # TODO: Add all other fields with appropriate types\n",
    "    # Consider: validation rules, optional fields, field descriptions\n",
    "```\n",
    "\n",
    "**Questions to consider:**\n",
    "- Which fields are required vs optional?\n",
    "- What validation rules should you add (e.g., date formats, risk rating values)?\n",
    "- How will you handle the SSN field for privacy?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941d818c",
   "metadata": {},
   "source": [
    "## ğŸ¦ Account Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf04e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore account data structure\n",
    "print(\"ğŸ“‹ Account Data Overview\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Shape: {accounts_df.shape}\")\n",
    "print(f\"Columns: {list(accounts_df.columns)}\")\n",
    "print(\"\\nğŸ“Š First few records:\")\n",
    "accounts_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ca6deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze account types and balances\n",
    "print(\"ğŸ’° Account Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Account types\n",
    "print(\"\\nğŸ¦ Account Types:\")\n",
    "account_types = accounts_df['account_type'].value_counts()\n",
    "print(account_types)\n",
    "\n",
    "# Balance analysis\n",
    "print(\"\\nğŸ’µ Balance Statistics:\")\n",
    "print(accounts_df[['current_balance', 'average_monthly_balance']].describe())\n",
    "\n",
    "# Visualize balance distributions\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "account_types.plot(kind='bar')\n",
    "plt.title('Account Types Distribution')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.hist(accounts_df['current_balance'], bins=50, alpha=0.7, color='blue')\n",
    "plt.title('Current Balance Distribution')\n",
    "plt.xlabel('Balance ($)')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.scatter(accounts_df['current_balance'], accounts_df['average_monthly_balance'], alpha=0.6)\n",
    "plt.title('Current vs Average Monthly Balance')\n",
    "plt.xlabel('Current Balance ($)')\n",
    "plt.ylabel('Average Monthly Balance ($)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9754c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze customer-account relationships\n",
    "print(\"ğŸ”— Customer-Account Relationships\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# How many accounts per customer?\n",
    "accounts_per_customer = accounts_df.groupby('customer_id').size()\n",
    "print(\"\\nğŸ“Š Accounts per Customer:\")\n",
    "print(accounts_per_customer.value_counts().sort_index())\n",
    "\n",
    "# Customers with multiple accounts\n",
    "multi_account_customers = accounts_per_customer[accounts_per_customer > 1]\n",
    "print(f\"\\nğŸ‘¥ Customers with multiple accounts: {len(multi_account_customers)}\")\n",
    "print(f\"ğŸ“ˆ Max accounts per customer: {accounts_per_customer.max()}\")\n",
    "\n",
    "# Visualize accounts per customer\n",
    "plt.figure(figsize=(10, 6))\n",
    "accounts_per_customer.value_counts().sort_index().plot(kind='bar')\n",
    "plt.title('Distribution of Accounts per Customer')\n",
    "plt.xlabel('Number of Accounts')\n",
    "plt.ylabel('Number of Customers')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cee698d",
   "metadata": {},
   "source": [
    "### ğŸ“ TODO: Design AccountData Schema\n",
    "\n",
    "Key considerations for your AccountData schema:\n",
    "- How will you link accounts to customers?\n",
    "- What validation is needed for balance fields?\n",
    "- How will you handle account status values?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8b932d",
   "metadata": {},
   "source": [
    "## ğŸ’³ Transaction Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d61c828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore transaction data structure\n",
    "print(\"ğŸ“‹ Transaction Data Overview\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Shape: {transactions_df.shape}\")\n",
    "print(f\"Columns: {list(transactions_df.columns)}\")\n",
    "print(\"\\nğŸ“Š First few records:\")\n",
    "transactions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21823926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert transaction_date to datetime for analysis\n",
    "transactions_df['transaction_date'] = pd.to_datetime(transactions_df['transaction_date'])\n",
    "\n",
    "print(\"ğŸ’° Transaction Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Transaction types\n",
    "print(\"\\nğŸ“Š Transaction Types:\")\n",
    "txn_types = transactions_df['transaction_type'].value_counts()\n",
    "print(txn_types)\n",
    "\n",
    "# Transaction methods\n",
    "print(\"\\nğŸ’³ Transaction Methods:\")\n",
    "txn_methods = transactions_df['method'].value_counts()\n",
    "print(txn_methods)\n",
    "\n",
    "# Amount statistics\n",
    "print(\"\\nğŸ’µ Transaction Amount Statistics:\")\n",
    "print(transactions_df['amount'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35841a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze transaction patterns for suspicious activity indicators\n",
    "print(\"ğŸš¨ Suspicious Activity Pattern Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Large transactions (potential money laundering)\n",
    "large_transactions = transactions_df[transactions_df['amount'] >= 100000]\n",
    "print(f\"\\nğŸ’° Large transactions (â‰¥$100k): {len(large_transactions)}\")\n",
    "\n",
    "# Transactions just under $10k (potential structuring)\n",
    "structuring_range = transactions_df[\n",
    "    (transactions_df['amount'] >= 9000) & \n",
    "    (transactions_df['amount'] < 10000)\n",
    "]\n",
    "print(f\"ğŸ”„ Potential structuring ($9k-$10k): {len(structuring_range)}\")\n",
    "\n",
    "# High-frequency customers (many transactions)\n",
    "txn_per_account = transactions_df.groupby('account_id').size()\n",
    "high_frequency_accounts = txn_per_account[txn_per_account >= 50]\n",
    "print(f\"ğŸ“ˆ High-frequency accounts (â‰¥50 txns): {len(high_frequency_accounts)}\")\n",
    "\n",
    "# Wire transfers (higher risk method)\n",
    "wire_transfers = transactions_df[transactions_df['method'] == 'Wire']\n",
    "print(f\"ğŸŒ Wire transfers: {len(wire_transfers)}\")\n",
    "\n",
    "print(f\"\\nâš ï¸ Potential Suspicious Indicators:\")\n",
    "print(f\"â€¢ {len(large_transactions)} large transactions requiring enhanced scrutiny\")\n",
    "print(f\"â€¢ {len(structuring_range)} transactions in structuring range\")\n",
    "print(f\"â€¢ {len(high_frequency_accounts)} accounts with high transaction frequency\")\n",
    "print(f\"â€¢ {len(wire_transfers)} wire transfers (higher risk method)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1d1f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize transaction patterns\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Transaction amounts distribution\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.hist(transactions_df['amount'], bins=50, alpha=0.7)\n",
    "plt.title('Transaction Amount Distribution')\n",
    "plt.xlabel('Amount ($)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.yscale('log')  # Log scale due to wide range\n",
    "\n",
    "# Transaction types\n",
    "plt.subplot(2, 3, 2)\n",
    "txn_types.plot(kind='bar')\n",
    "plt.title('Transaction Types')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Transaction methods\n",
    "plt.subplot(2, 3, 3)\n",
    "txn_methods.plot(kind='bar')\n",
    "plt.title('Transaction Methods')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Daily transaction volume\n",
    "plt.subplot(2, 3, 4)\n",
    "daily_volume = transactions_df.groupby(transactions_df['transaction_date'].dt.date)['amount'].sum()\n",
    "daily_volume.plot()\n",
    "plt.title('Daily Transaction Volume')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Transaction count per account\n",
    "plt.subplot(2, 3, 5)\n",
    "plt.hist(txn_per_account, bins=30, alpha=0.7)\n",
    "plt.title('Transactions per Account')\n",
    "plt.xlabel('Number of Transactions')\n",
    "plt.ylabel('Number of Accounts')\n",
    "\n",
    "# Structuring analysis\n",
    "plt.subplot(2, 3, 6)\n",
    "amount_ranges = pd.cut(transactions_df['amount'], \n",
    "                      bins=[0, 5000, 9000, 10000, 50000, float('inf')],\n",
    "                      labels=['<$5k', '$5k-$9k', '$9k-$10k', '$10k-$50k', '>$50k'])\n",
    "amount_ranges.value_counts().plot(kind='bar')\n",
    "plt.title('Transaction Amount Ranges (Structuring Analysis)')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f91ba45",
   "metadata": {},
   "source": [
    "## ğŸ”— Data Relationships Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d30c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the complete data relationship chain\n",
    "print(\"ğŸ”— Complete Data Relationship Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Join all datasets to understand relationships\n",
    "# First join accounts with customers\n",
    "customer_accounts = accounts_df.merge(customers_df, on='customer_id', how='left')\n",
    "\n",
    "# Then join with transactions\n",
    "full_data = transactions_df.merge(customer_accounts, on='account_id', how='left')\n",
    "\n",
    "print(f\"\\nğŸ“Š Combined dataset shape: {full_data.shape}\")\n",
    "print(f\"âœ… Successful joins: {len(full_data[full_data['customer_id'].notna()])} / {len(full_data)}\")\n",
    "\n",
    "# Check for any orphaned records\n",
    "orphaned_transactions = full_data[full_data['customer_id'].isna()]\n",
    "if len(orphaned_transactions) > 0:\n",
    "    print(f\"âš ï¸ Found {len(orphaned_transactions)} orphaned transactions (no matching account/customer)\")\n",
    "else:\n",
    "    print(\"âœ… All transactions have valid account and customer relationships\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff55721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze high-risk customer activity\n",
    "print(\"âš ï¸ High-Risk Customer Activity Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Filter for high-risk customers\n",
    "high_risk_data = full_data[full_data['risk_rating'] == 'High']\n",
    "\n",
    "print(f\"\\nğŸ“Š High-risk customer transactions: {len(high_risk_data)}\")\n",
    "print(f\"ğŸ’° Total high-risk transaction volume: ${high_risk_data['amount'].sum():,.2f}\")\n",
    "print(f\"ğŸ“ˆ Average transaction amount: ${high_risk_data['amount'].mean():,.2f}\")\n",
    "\n",
    "# High-risk customers with large transactions\n",
    "high_risk_large = high_risk_data[high_risk_data['amount'] >= 50000]\n",
    "print(f\"\\nğŸš¨ High-risk customers with large transactions (â‰¥$50k): {len(high_risk_large)}\")\n",
    "\n",
    "if len(high_risk_large) > 0:\n",
    "    print(\"\\nğŸ” Sample high-risk large transactions:\")\n",
    "    print(high_risk_large[['name', 'amount', 'transaction_type', 'method', 'transaction_date']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a76de90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify potential cases for SAR processing\n",
    "print(\"ğŸ¯ Potential SAR Cases Identification\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Define criteria for suspicious cases\n",
    "def identify_suspicious_cases(df):\n",
    "    suspicious_criteria = {\n",
    "        'high_risk_customers': df['risk_rating'] == 'High',\n",
    "        'large_transactions': df['amount'] >= 100000,\n",
    "        'high_frequency': df.groupby('customer_id')['transaction_id'].transform('count') >= 50,\n",
    "        'wire_transfers': df['method'] == 'Wire'\n",
    "    }\n",
    "    \n",
    "    # Apply any of the criteria\n",
    "    suspicious_mask = (\n",
    "        suspicious_criteria['high_risk_customers'] |\n",
    "        suspicious_criteria['large_transactions'] |\n",
    "        suspicious_criteria['high_frequency'] |\n",
    "        suspicious_criteria['wire_transfers']\n",
    "    )\n",
    "    \n",
    "    return df[suspicious_mask], suspicious_criteria\n",
    "\n",
    "suspicious_data, criteria = identify_suspicious_cases(full_data)\n",
    "\n",
    "print(f\"\\nğŸ“Š Suspicious transactions identified: {len(suspicious_data)}\")\n",
    "print(f\"ğŸ‘¥ Unique customers involved: {suspicious_data['customer_id'].nunique()}\")\n",
    "\n",
    "# Break down by criteria\n",
    "print(\"\\nğŸ” Breakdown by criteria:\")\n",
    "for criterion, mask in criteria.items():\n",
    "    count = full_data[mask]['customer_id'].nunique()\n",
    "    print(f\"â€¢ {criterion.replace('_', ' ').title()}: {count} customers\")\n",
    "\n",
    "# Top customers by transaction volume\n",
    "customer_volumes = suspicious_data.groupby(['customer_id', 'name'])['amount'].agg(['sum', 'count']).reset_index()\n",
    "customer_volumes = customer_volumes.sort_values('sum', ascending=False)\n",
    "\n",
    "print(\"\\nğŸ’° Top 5 customers by suspicious transaction volume:\")\n",
    "print(customer_volumes.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390b45f8",
   "metadata": {},
   "source": [
    "## ğŸ“ Key Insights and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a153b8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of key insights for schema design\n",
    "print(\"ğŸ” DATA EXPLORATION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nğŸ“Š Dataset Overview:\")\n",
    "print(f\"â€¢ {len(customers_df)} customers ({customers_df['risk_rating'].value_counts()['High']} high-risk)\")\n",
    "print(f\"â€¢ {len(accounts_df)} accounts ({accounts_df['account_type'].nunique()} types)\")\n",
    "print(f\"â€¢ {len(transactions_df)} transactions (${transactions_df['amount'].sum():,.2f} total volume)\")\n",
    "\n",
    "print(\"\\nğŸš¨ Suspicious Activity Indicators:\")\n",
    "print(f\"â€¢ {len(large_transactions)} large transactions (â‰¥$100k)\")\n",
    "print(f\"â€¢ {len(structuring_range)} potential structuring transactions ($9k-$10k)\")\n",
    "print(f\"â€¢ {len(high_frequency_accounts)} high-frequency accounts (â‰¥50 transactions)\")\n",
    "print(f\"â€¢ {suspicious_data['customer_id'].nunique()} customers with suspicious patterns\")\n",
    "\n",
    "print(\"\\nğŸ—ï¸ Schema Design Requirements:\")\n",
    "print(\"â€¢ CustomerData: Handle optional fields, validate risk ratings\")\n",
    "print(\"â€¢ AccountData: Link to customers, validate balances\")\n",
    "print(\"â€¢ TransactionData: Validate amounts, handle date formats\")\n",
    "print(\"â€¢ CaseData: Combine all entities with metadata\")\n",
    "\n",
    "print(\"\\nğŸ¯ Next Steps:\")\n",
    "print(\"1. Implement Pydantic schemas in src/foundation_sar.py\")\n",
    "print(\"2. Create DataLoader to build unified case objects\")\n",
    "print(\"3. Add comprehensive data validation\")\n",
    "print(\"4. Test with the suspicious cases identified above\")\n",
    "\n",
    "print(\"\\nğŸš€ Ready to move to Phase 2: Agent Development!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7f51a0",
   "metadata": {},
   "source": [
    "## ğŸ“‹ TODO: Implementation Checklist\n",
    "\n",
    "Based on your data exploration, complete these tasks in `src/foundation_sar.py`:\n",
    "\n",
    "### âœ… Pydantic Schemas\n",
    "- [ ] `CustomerData` - Include all customer fields with proper validation\n",
    "- [ ] `AccountData` - Include account details with balance validation\n",
    "- [ ] `TransactionData` - Include transaction fields with amount/date validation\n",
    "- [ ] `CaseData` - Combine customer, accounts, and transactions\n",
    "- [ ] `RiskAnalystOutput` - Structure for risk analysis results\n",
    "- [ ] `ComplianceOfficerOutput` - Structure for compliance narratives\n",
    "\n",
    "### âœ… Utility Classes\n",
    "- [ ] `ExplainabilityLogger` - Audit trail logging system\n",
    "- [ ] `DataLoader` - Case object creation from CSV data\n",
    "\n",
    "### âœ… Testing\n",
    "- [ ] Test schemas with the actual data from this exploration\n",
    "- [ ] Validate that all suspicious cases can be processed\n",
    "- [ ] Ensure proper error handling for edge cases\n",
    "\n",
    "**ğŸ’¡ Pro Tip**: Use the insights from this exploration to guide your validation rules. For example, you now know that risk ratings are 'Low', 'Medium', or 'High', so you can use a Literal type for validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d90057",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ§ª Testing Your Implementation\n",
    "\n",
    "Once you've implemented the schemas in `src/foundation_sar.py`, validate your work with this two-step testing procedure:\n",
    "\n",
    "### Step 1: Smoke Test - Basic Import and Instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0117ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ§ª STEP 1: Smoke Test - Basic Import and Instantiation\n",
    "# This test verifies that your classes can be imported and basic objects created\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add src directory to Python path so we can import our modules\n",
    "project_root = os.path.abspath('..')  # Go up one level from notebooks to starter\n",
    "src_path = os.path.join(project_root, 'src')\n",
    "sys.path.insert(0, src_path)\n",
    "\n",
    "print(f\"ğŸ“ Added to Python path: {src_path}\")\n",
    "print(f\"ğŸ” Current working directory: {os.getcwd()}\")\n",
    "\n",
    "try:\n",
    "    # Import your implemented classes\n",
    "    from foundation_sar import (\n",
    "        CustomerData,\n",
    "        AccountData, \n",
    "        TransactionData,\n",
    "        CaseData,\n",
    "        RiskAnalystOutput,\n",
    "        ComplianceOfficerOutput,\n",
    "        ExplainabilityLogger,\n",
    "        DataLoader\n",
    "    )\n",
    "    \n",
    "    print(\"âœ… All classes imported successfully!\")\n",
    "    \n",
    "    # Test basic instantiation with minimal data\n",
    "    print(\"\\nğŸ§ª Testing basic object creation...\")\n",
    "    \n",
    "    # Test CustomerData\n",
    "    customer = CustomerData(\n",
    "        customer_id=\"TEST_001\",\n",
    "        name=\"Test Customer\",\n",
    "        date_of_birth=\"1990-01-01\",\n",
    "        ssn_last_4=\"1234\",\n",
    "        address=\"123 Test St\",\n",
    "        customer_since=\"2020-01-01\",\n",
    "        risk_rating=\"Low\"\n",
    "    )\n",
    "    print(f\"âœ… CustomerData created: {customer.name}\")\n",
    "    \n",
    "    # Test AccountData  \n",
    "    account = AccountData(\n",
    "        account_id=\"TEST_001_ACC_1\",\n",
    "        customer_id=\"TEST_001\",\n",
    "        account_type=\"Checking\",\n",
    "        opening_date=\"2020-01-01\",\n",
    "        current_balance=1000.0,\n",
    "        average_monthly_balance=800.0,\n",
    "        status=\"Active\"\n",
    "    )\n",
    "    print(f\"âœ… AccountData created: {account.account_type}\")\n",
    "    \n",
    "    # Test TransactionData\n",
    "    transaction = TransactionData(\n",
    "        transaction_id=\"TXN_TEST_001\",\n",
    "        account_id=\"TEST_001_ACC_1\",\n",
    "        transaction_date=\"2025-01-01\",\n",
    "        transaction_type=\"Cash_Deposit\",\n",
    "        amount=500.0,\n",
    "        description=\"Test transaction\",\n",
    "        method=\"Cash\"\n",
    "    )\n",
    "    print(f\"âœ… TransactionData created: ${transaction.amount}\")\n",
    "    \n",
    "    # Test utility classes\n",
    "    logger = ExplainabilityLogger(\"smoke_test.jsonl\")\n",
    "    loader = DataLoader(logger)\n",
    "    print(\"âœ… ExplainabilityLogger and DataLoader created\")\n",
    "    \n",
    "    print(\"\\nğŸ‰ SMOKE TEST PASSED! Your basic implementation is working.\")\n",
    "    print(\"ğŸ“ Now proceed to Step 2 for comprehensive testing.\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"âŒ Import Error: {e}\")\n",
    "    print(\"ğŸ’¡ Make sure you've implemented all classes in src/foundation_sar.py\")\n",
    "    print(\"ğŸ“ Classes needed: CustomerData, AccountData, TransactionData, CaseData,\")\n",
    "    print(\"   RiskAnalystOutput, ComplianceOfficerOutput, ExplainabilityLogger, DataLoader\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error creating objects: {e}\")\n",
    "    print(\"ğŸ’¡ Check your Pydantic schema implementations for required fields and validation\")\n",
    "    \n",
    "finally:\n",
    "    # Cleanup test file if created\n",
    "    if os.path.exists(\"smoke_test.jsonl\"):\n",
    "        os.remove(\"smoke_test.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1e2ce3",
   "metadata": {},
   "source": [
    "### Step 2: Full Test Suite - Comprehensive Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa56173f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ§ª STEP 2: Full Test Suite - Comprehensive Validation\n",
    "# This runs the complete test suite to validate your implementation\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Ensure path is set up (in case this cell is run independently)\n",
    "project_root = os.path.abspath('..')\n",
    "src_path = os.path.join(project_root, 'src')\n",
    "tests_path = os.path.join(project_root, 'tests')\n",
    "\n",
    "if src_path not in sys.path:\n",
    "    sys.path.insert(0, src_path)\n",
    "if tests_path not in sys.path:\n",
    "    sys.path.insert(0, tests_path)\n",
    "\n",
    "print(f\"ğŸ“ Python path includes:\")\n",
    "print(f\"   ğŸ“‚ src: {src_path}\")\n",
    "print(f\"   ğŸ“‚ tests: {tests_path}\")\n",
    "\n",
    "try:\n",
    "    # Import and run tests from the test suite\n",
    "    from test_foundation import (\n",
    "        TestCustomerData,\n",
    "        TestAccountData,\n",
    "        TestTransactionData,\n",
    "        TestCaseData,\n",
    "        TestDataLoader,\n",
    "        TestExplainabilityLogger\n",
    "    )\n",
    "    \n",
    "    print(\"âœ… Test classes imported successfully!\")\n",
    "    print(\"\\nğŸ§ª Running comprehensive test suite...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Initialize test instances\n",
    "    test_customer = TestCustomerData()\n",
    "    test_account = TestAccountData()\n",
    "    test_transaction = TestTransactionData()\n",
    "    test_case = TestCaseData()\n",
    "    test_loader = TestDataLoader()\n",
    "    test_logger = TestExplainabilityLogger()\n",
    "    \n",
    "    tests_passed = 0\n",
    "    tests_failed = 0\n",
    "    \n",
    "    # Run CustomerData tests\n",
    "    print(\"\\nğŸ‘¥ Testing CustomerData...\")\n",
    "    try:\n",
    "        test_customer.test_valid_customer_data()\n",
    "        print(\"  âœ… Valid customer data test passed\")\n",
    "        tests_passed += 1\n",
    "    except Exception as e:\n",
    "        print(f\"  âŒ Valid customer data test failed: {e}\")\n",
    "        tests_failed += 1\n",
    "        \n",
    "    try:\n",
    "        test_customer.test_customer_risk_rating_validation()\n",
    "        print(\"  âœ… Risk rating validation test passed\")\n",
    "        tests_passed += 1\n",
    "    except Exception as e:\n",
    "        print(f\"  âŒ Risk rating validation test failed: {e}\")\n",
    "        tests_failed += 1\n",
    "    \n",
    "    # Run AccountData tests\n",
    "    print(\"\\nğŸ¦ Testing AccountData...\")\n",
    "    try:\n",
    "        test_account.test_valid_account_data()\n",
    "        print(\"  âœ… Valid account data test passed\")\n",
    "        tests_passed += 1\n",
    "    except Exception as e:\n",
    "        print(f\"  âŒ Valid account data test failed: {e}\")\n",
    "        tests_failed += 1\n",
    "        \n",
    "    try:\n",
    "        test_account.test_account_balance_validation()\n",
    "        print(\"  âœ… Account balance validation test passed\")\n",
    "        tests_passed += 1\n",
    "    except Exception as e:\n",
    "        print(f\"  âŒ Account balance validation test failed: {e}\")\n",
    "        tests_failed += 1\n",
    "    \n",
    "    # Run TransactionData tests\n",
    "    print(\"\\nğŸ’³ Testing TransactionData...\")\n",
    "    try:\n",
    "        test_transaction.test_valid_transaction_data()\n",
    "        print(\"  âœ… Valid transaction data test passed\")\n",
    "        tests_passed += 1\n",
    "    except Exception as e:\n",
    "        print(f\"  âŒ Valid transaction data test failed: {e}\")\n",
    "        tests_failed += 1\n",
    "        \n",
    "    try:\n",
    "        test_transaction.test_transaction_amount_validation()\n",
    "        print(\"  âœ… Transaction amount validation test passed\")\n",
    "        tests_passed += 1\n",
    "    except Exception as e:\n",
    "        print(f\"  âŒ Transaction amount validation test failed: {e}\")\n",
    "        tests_failed += 1\n",
    "    \n",
    "    # Run CaseData tests\n",
    "    print(\"\\nğŸ“‹ Testing CaseData...\")\n",
    "    try:\n",
    "        test_case.test_valid_case_creation()\n",
    "        print(\"  âœ… Valid case creation test passed\")\n",
    "        tests_passed += 1\n",
    "    except Exception as e:\n",
    "        print(f\"  âŒ Valid case creation test failed: {e}\")\n",
    "        tests_failed += 1\n",
    "    \n",
    "    # Run DataLoader tests\n",
    "    print(\"\\nğŸ“Š Testing DataLoader...\")\n",
    "    try:\n",
    "        test_loader.test_csv_data_loading()\n",
    "        print(\"  âœ… CSV data loading test passed\")\n",
    "        tests_passed += 1\n",
    "    except Exception as e:\n",
    "        print(f\"  âŒ CSV data loading test failed: {e}\")\n",
    "        tests_failed += 1\n",
    "    \n",
    "    # Run ExplainabilityLogger tests\n",
    "    print(\"\\nğŸ“ Testing ExplainabilityLogger...\")\n",
    "    try:\n",
    "        test_logger.test_log_creation()\n",
    "        print(\"  âœ… Log creation test passed\")\n",
    "        tests_passed += 1\n",
    "    except Exception as e:\n",
    "        print(f\"  âŒ Log creation test failed: {e}\")\n",
    "        tests_failed += 1\n",
    "        \n",
    "    try:\n",
    "        test_logger.test_log_file_writing()\n",
    "        print(\"  âœ… Log file writing test passed\")\n",
    "        tests_passed += 1\n",
    "    except Exception as e:\n",
    "        print(f\"  âŒ Log file writing test failed: {e}\")\n",
    "        tests_failed += 1\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(f\"ğŸ“Š TEST SUMMARY:\")\n",
    "    print(f\"âœ… Tests Passed: {tests_passed}\")\n",
    "    print(f\"âŒ Tests Failed: {tests_failed}\")\n",
    "    print(f\"ğŸ“ˆ Success Rate: {tests_passed/(tests_passed+tests_failed)*100:.1f}%\")\n",
    "    \n",
    "    if tests_failed == 0:\n",
    "        print(\"\\nğŸ‰ CONGRATULATIONS! All tests passed!\")\n",
    "        print(\"ğŸš€ Your foundation_sar.py implementation is complete and ready.\")\n",
    "        print(\"ğŸ“ You can now proceed to Phase 2: Agent Development.\")\n",
    "    else:\n",
    "        print(f\"\\nâš ï¸  {tests_failed} tests failed. Review the error messages above.\")\n",
    "        print(\"ğŸ’¡ Common issues:\")\n",
    "        print(\"   â€¢ Missing required fields in Pydantic schemas\")\n",
    "        print(\"   â€¢ Incorrect field types or validation rules\")\n",
    "        print(\"   â€¢ Missing validation decorators\")\n",
    "        print(\"   â€¢ Incomplete method implementations\")\n",
    "        print(\"ğŸ“ Fix the issues and re-run this cell.\")\n",
    "        \n",
    "except ImportError as e:\n",
    "    print(f\"âŒ Import Error: {e}\")\n",
    "    print(\"ğŸ’¡ Make sure your implementation is complete and error-free\")\n",
    "    print(\"ğŸ“ Run Step 1 first to check basic imports\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Unexpected error: {e}\")\n",
    "    print(\"ğŸ’¡ Check your implementation for syntax or logic errors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b919f0c",
   "metadata": {},
   "source": [
    "## âœ… Next Steps After Successful Testing\n",
    "\n",
    "Once both testing steps pass:\n",
    "\n",
    "### ğŸ¯ Immediate Actions\n",
    "1. **Verify** all test output shows âœ… passed status\n",
    "2. **Review** any warning messages for potential improvements  \n",
    "3. **Check** that audit log files are created properly\n",
    "\n",
    "### ğŸš€ Ready for Phase 2: Agent Development\n",
    "Navigate to `notebooks/02_agent_development.ipynb` to:\n",
    "- Build the Risk Analyst Agent (Chain-of-Thought prompting)\n",
    "- Implement the Compliance Officer Agent (ReACT framework)\n",
    "- Test agents with the cases identified in this exploration\n",
    "\n",
    "### ğŸ”§ Troubleshooting Common Issues\n",
    "- **Import Errors**: Ensure `src/foundation_sar.py` has no syntax errors\n",
    "- **Validation Errors**: Check Pydantic field types match the data types from exploration\n",
    "- **Missing Methods**: Verify all TODO sections are implemented, not just `pass` statements\n",
    "\n",
    "### ğŸ“š Reference\n",
    "- Your data exploration insights from this notebook\n",
    "- Implementation checklist above\n",
    "- Test results from Step 2 validation\n",
    "\n",
    "**ğŸ’¡ Pro Tip**: Keep this notebook open for reference - the data patterns you discovered here will guide your agent prompting strategies in Phase 2!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
